{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funathon 2023 - Sujet 5\n",
    "\n",
    "Analyse textuelle des commentaires clients de services de commande de repas en ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /opt/mamba/lib/python3.10/site-packages (from -r ../requirements.txt (line 1)) (0.0.1)\n",
      "Requirement already satisfied: datetime in /opt/mamba/lib/python3.10/site-packages (from -r ../requirements.txt (line 2)) (5.1)\n",
      "Requirement already satisfied: matplotlib in /opt/mamba/lib/python3.10/site-packages (from -r ../requirements.txt (line 3)) (3.7.1)\n",
      "Requirement already satisfied: nltk in /opt/mamba/lib/python3.10/site-packages (from -r ../requirements.txt (line 4)) (3.8.1)\n",
      "Requirement already satisfied: pandas in /opt/mamba/lib/python3.10/site-packages (from -r ../requirements.txt (line 5)) (2.0.2)\n",
      "Requirement already satisfied: pyLDAvis in /opt/mamba/lib/python3.10/site-packages (from -r ../requirements.txt (line 6)) (3.4.1)\n",
      "Requirement already satisfied: requests in /opt/mamba/lib/python3.10/site-packages (from -r ../requirements.txt (line 7)) (2.31.0)\n",
      "Requirement already satisfied: spacy in /opt/mamba/lib/python3.10/site-packages (from -r ../requirements.txt (line 8)) (3.5.3)\n",
      "Requirement already satisfied: tqdm in /opt/mamba/lib/python3.10/site-packages (from -r ../requirements.txt (line 9)) (4.65.0)\n",
      "Requirement already satisfied: unidecode in /opt/mamba/lib/python3.10/site-packages (from -r ../requirements.txt (line 10)) (1.3.6)\n",
      "Requirement already satisfied: wordcloud in /opt/mamba/lib/python3.10/site-packages (from -r ../requirements.txt (line 11)) (1.9.2)\n",
      "Requirement already satisfied: transformers in /opt/mamba/lib/python3.10/site-packages (from -r ../requirements.txt (line 12)) (4.30.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/mamba/lib/python3.10/site-packages (from -r ../requirements.txt (line 13)) (1.2.2)\n",
      "Requirement already satisfied: pyarrow in /opt/mamba/lib/python3.10/site-packages (from -r ../requirements.txt (line 14)) (12.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/mamba/lib/python3.10/site-packages (from bs4->-r ../requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: zope.interface in /opt/mamba/lib/python3.10/site-packages (from datetime->-r ../requirements.txt (line 2)) (6.0)\n",
      "Requirement already satisfied: pytz in /opt/mamba/lib/python3.10/site-packages (from datetime->-r ../requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 3)) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 3)) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 3)) (1.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 3)) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 3)) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: click in /opt/mamba/lib/python3.10/site-packages (from nltk->-r ../requirements.txt (line 4)) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/mamba/lib/python3.10/site-packages (from nltk->-r ../requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/mamba/lib/python3.10/site-packages (from nltk->-r ../requirements.txt (line 4)) (2023.6.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/mamba/lib/python3.10/site-packages (from pandas->-r ../requirements.txt (line 5)) (2023.3)\n",
      "Requirement already satisfied: scipy in /opt/mamba/lib/python3.10/site-packages (from pyLDAvis->-r ../requirements.txt (line 6)) (1.11.0)\n",
      "Requirement already satisfied: jinja2 in /opt/mamba/lib/python3.10/site-packages (from pyLDAvis->-r ../requirements.txt (line 6)) (3.1.2)\n",
      "Requirement already satisfied: numexpr in /opt/mamba/lib/python3.10/site-packages (from pyLDAvis->-r ../requirements.txt (line 6)) (2.8.4)\n",
      "Requirement already satisfied: funcy in /opt/mamba/lib/python3.10/site-packages (from pyLDAvis->-r ../requirements.txt (line 6)) (2.0)\n",
      "Requirement already satisfied: gensim in /opt/mamba/lib/python3.10/site-packages (from pyLDAvis->-r ../requirements.txt (line 6)) (4.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/mamba/lib/python3.10/site-packages (from pyLDAvis->-r ../requirements.txt (line 6)) (65.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.10/site-packages (from requests->-r ../requirements.txt (line 7)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.10/site-packages (from requests->-r ../requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.10/site-packages (from requests->-r ../requirements.txt (line 7)) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.10/site-packages (from requests->-r ../requirements.txt (line 7)) (2023.5.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/mamba/lib/python3.10/site-packages (from spacy->-r ../requirements.txt (line 8)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/mamba/lib/python3.10/site-packages (from spacy->-r ../requirements.txt (line 8)) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/mamba/lib/python3.10/site-packages (from spacy->-r ../requirements.txt (line 8)) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/mamba/lib/python3.10/site-packages (from spacy->-r ../requirements.txt (line 8)) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/mamba/lib/python3.10/site-packages (from spacy->-r ../requirements.txt (line 8)) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/mamba/lib/python3.10/site-packages (from spacy->-r ../requirements.txt (line 8)) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/mamba/lib/python3.10/site-packages (from spacy->-r ../requirements.txt (line 8)) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/mamba/lib/python3.10/site-packages (from spacy->-r ../requirements.txt (line 8)) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/mamba/lib/python3.10/site-packages (from spacy->-r ../requirements.txt (line 8)) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/mamba/lib/python3.10/site-packages (from spacy->-r ../requirements.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/mamba/lib/python3.10/site-packages (from spacy->-r ../requirements.txt (line 8)) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/mamba/lib/python3.10/site-packages (from spacy->-r ../requirements.txt (line 8)) (6.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/mamba/lib/python3.10/site-packages (from spacy->-r ../requirements.txt (line 8)) (1.10.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/mamba/lib/python3.10/site-packages (from spacy->-r ../requirements.txt (line 8)) (3.3.0)\n",
      "Requirement already satisfied: filelock in /opt/mamba/lib/python3.10/site-packages (from transformers->-r ../requirements.txt (line 12)) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/mamba/lib/python3.10/site-packages (from transformers->-r ../requirements.txt (line 12)) (0.15.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/mamba/lib/python3.10/site-packages (from transformers->-r ../requirements.txt (line 12)) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/mamba/lib/python3.10/site-packages (from transformers->-r ../requirements.txt (line 12)) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/mamba/lib/python3.10/site-packages (from transformers->-r ../requirements.txt (line 12)) (0.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/mamba/lib/python3.10/site-packages (from scikit-learn->-r ../requirements.txt (line 13)) (3.1.0)\n",
      "Requirement already satisfied: fsspec in /opt/mamba/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r ../requirements.txt (line 12)) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/mamba/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r ../requirements.txt (line 12)) (4.6.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r ../requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/mamba/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy->-r ../requirements.txt (line 8)) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/mamba/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy->-r ../requirements.txt (line 8)) (0.0.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/mamba/lib/python3.10/site-packages (from beautifulsoup4->bs4->-r ../requirements.txt (line 1)) (2.3.2.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.10/site-packages (from jinja2->pyLDAvis->-r ../requirements.txt (line 6)) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"https://minio.lab.sspcloud.fr/projet-funathon/2023/sujet5/diffusion/reviews_takeaway.parquet\")\n",
    "\n",
    "# local copy of the data\n",
    "df.to_parquet(\"reviews_takeaway.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"reviews_takeaway.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data à InputExample format\n",
    "class InputExample(object):\n",
    "    def __init__(self, guid, text_a, text_b, label):\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "def convert_data_to_examples(df, input_column, target_column):\n",
    "    examples = []\n",
    "    for i, row in df.iterrows():\n",
    "        guid = None\n",
    "        text_a = row[input_column]\n",
    "        text_b = None\n",
    "        label = row[target_column]\n",
    "        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    return examples\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, examples, tokenizer, max_length):\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.examples[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            example.text_a,\n",
    "            example.text_b,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids = torch.tensor(inputs[\"input_ids\"])\n",
    "        attention_mask = torch.tensor(inputs[\"attention_mask\"])\n",
    "        token_type_ids = torch.tensor(inputs[\"token_type_ids\"])\n",
    "        label = torch.tensor(example.label)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"label\": label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "df['full_raw'] = 'TITRE: ' + df['title'] + ' |AND| ' + 'COMMENT: ' + df['comment']\n",
    "df_text = df[['note', 'full_raw']].copy()\n",
    "df_text['note'] = df_text['note'] - 1\n",
    "\n",
    "train_df, val_df = train_test_split(df_text, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert à InputExample format\n",
    "train_examples = convert_data_to_examples(train_df, 'full_raw', 'note')\n",
    "val_examples = convert_data_to_examples(val_df, 'full_raw', 'note')\n",
    "test_examples = convert_data_to_examples(test_df, 'full_raw', 'note')\n",
    "\n",
    "# Convert à PyTorch dataset format\n",
    "train_dataset = CustomDataset(train_examples, tokenizer, max_length=128)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(val_examples, tokenizer, max_length=128)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "test_dataset = CustomDataset(test_examples, tokenizer, max_length=128)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "NUM_EPOCHS = 3 #à 10 epochs, ça overfit dès le 4e epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=5)\n",
    "\n",
    "# Freeze BERT layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Define optimizer, loss function, and metric\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, eps=1e-8)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=model.to(device)\n",
    "\n",
    "\n",
    "#for epoch in range(NUM_EPOCHS): #si sans tqdm\n",
    "for epoch in tqdm(range(NUM_EPOCHS), desc=\"Training\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    train_loss = total_loss / len(train_dataloader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_samples += labels.size(0)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_accuracy = val_correct / val_samples\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "test_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_samples += labels.size(0)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_dataloader)\n",
    "test_accuracy = test_correct / test_samples\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='TITRE: JE SUIS PAS CONTENT !!! ' ' |AND| ' + 'COMMENT: VOTRE PRODUIT EST NUL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        guid = None\n",
    "        text_a = row[input_column]\n",
    "        text_b = None\n",
    "        label = row[target_column]\n",
    "        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
