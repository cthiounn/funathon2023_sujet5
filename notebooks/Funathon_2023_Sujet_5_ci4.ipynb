{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funathon 2023 - Sujet 5\n",
    "\n",
    "Analyse textuelle des commentaires clients de services de commande de repas en ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# download the data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"https://minio.lab.sspcloud.fr/projet-funathon/2023/sujet5/diffusion/reviews_takeaway.parquet\")\n",
    "\n",
    "# local copy of the data\n",
    "df.to_parquet(\"reviews_takeaway.parquet\")\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"reviews_takeaway.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame info\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commentaires par note\n",
    "\n",
    "# Definir le nombre de samples par note\n",
    "samples_per_note = 1\n",
    "\n",
    "\n",
    "grouped_df = df.groupby('note')\n",
    "selected_indexes = []\n",
    "\n",
    "for note, group in grouped_df:\n",
    "    indexes = random.sample(group.index.tolist(), k=samples_per_note)\n",
    "    selected_indexes.extend(indexes)\n",
    "\n",
    "\n",
    "for index in selected_indexes:\n",
    "    note = df.loc[index, 'note']\n",
    "    title = df.loc[index, 'title']\n",
    "    comment = df.loc[index, 'comment']\n",
    "    \n",
    "    print(\"Note\", note, \"comments:\")\n",
    "    print(\"Title:\", title)\n",
    "    \n",
    "    wrapped_comment = textwrap.fill(comment, width=80)\n",
    "    print(\"Comment:\")\n",
    "    print(wrapped_comment)\n",
    "    \n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day, day_name, month, year\n",
    "\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['day'] = df['date'].dt.day\n",
    "df['day_name'] = df['date'].dt.day_name()\n",
    "df['month'] = df['date'].dt.month\n",
    "df['month_name'] = df['date'].dt.month_name()\n",
    "\n",
    "# Define a function to map month values to seasons\n",
    "def get_season(month):\n",
    "    if 3 <= month <= 5:\n",
    "        return 'Spring'\n",
    "    elif 6 <= month <= 8:\n",
    "        return 'Summer'\n",
    "    elif 9 <= month <= 11:\n",
    "        return 'Autumn'\n",
    "    else:\n",
    "        return 'Winter'\n",
    "\n",
    "df['season'] = df['month'].apply(get_season)\n",
    "\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Add new features for the length of 'title' and 'comment'\n",
    "df['title_len'] = df['title'].apply(len)\n",
    "df['comment_len'] = df['comment'].apply(len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exploratory data analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count distinct noms de company\n",
    "distinct_company_count = df['company'].nunique()\n",
    "print(\"Count of distinct company names:\", distinct_company_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List distinct noms de company\n",
    "distinct_company_list = df['company'].unique().tolist()\n",
    "print(\"List of distinct company names:\", distinct_company_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Num of observation by features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List des features pour histograms\n",
    "features = ['note' , 'company', 'hour', 'day', 'day_name', 'month', 'month_name', 'season', 'year']\n",
    "\n",
    "# histograms\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    df[feature].value_counts().plot(kind='bar')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram de {}'.format(feature))\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features pour boxplots\n",
    "features = ['company', 'hour', 'day', 'day_name', 'month', 'month_name', 'season', 'year']\n",
    "\n",
    "# Boxplots\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = df.boxplot(column='note', by=feature, widths=0.6)\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Note')\n",
    "    ax.set_title('Boxplot de Note par {}'.format(feature))\n",
    "    \n",
    "    # Ajout du nombre d'observations\n",
    "    counts = df.groupby(feature)['note'].count().values\n",
    "    positions = range(1, len(counts) + 1)\n",
    "    for pos, count in zip(positions, counts):\n",
    "        ax.annotate(str(count), xy=(pos, df['note'].max()), xytext=(0, 3),\n",
    "                    textcoords='offset points', ha='center', va='bottom', rotation='vertical', fontsize=8)\n",
    "    \n",
    "    # Mean point pour les modalités\n",
    "    means = df.groupby(feature)['note'].mean()\n",
    "    x_values = [pos for pos in positions]\n",
    "    y_values = means.values\n",
    "    ax.plot(x_values, y_values, marker='o', linestyle='', markersize=5, color='red')\n",
    "    \n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.ylim(0, 5.9)  # Set y-axis range\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP & Sentiment Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords update\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Fonction pour preprocesser le text\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Retirer les nombres\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text, language='french')\n",
    "    \n",
    "    # Retirer les stopwords\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Tokens sous format 'single string'\n",
    "    preprocessed_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Preprocesse 'title'\n",
    "df['title_preprocessed'] = df['title'].apply(preprocess_text)\n",
    "\n",
    "# Preprocesse 'comment'\n",
    "df['comment_preprocessed'] = df['comment'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[['title', 'title_preprocessed', 'comment', 'comment_preprocessed']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['full_text'] = df['title_preprocessed'] + ' ' + df['comment_preprocessed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Comparer texte brut et text processé\n",
    "for index in selected_indexes:\n",
    "    note = df.loc[index, 'note']\n",
    "    title = df.loc[index, 'title']\n",
    "    comment = df.loc[index, 'comment']\n",
    "    processed_title = df.loc[index, 'title_preprocessed']\n",
    "    processed_comment = df.loc[index, 'comment_preprocessed']\n",
    "    \n",
    "    print(\"Note\", note, \"comments:\")\n",
    "    print(\"Unprocessed Title:\", title, \"\\n\")\n",
    "    print(\"Processed Title:\", processed_title, \"\\n\", \"\\n\")\n",
    "    \n",
    "    wrapped_comment = textwrap.fill(comment, width=80)\n",
    "    print(\"Unprocessed Comment:\")\n",
    "    print(wrapped_comment, \"\\n\")\n",
    "    \n",
    "    wrapped_pcomment = textwrap.fill(processed_comment, width=80)\n",
    "    print(\"Processed Comment:\")\n",
    "    print(wrapped_pcomment, \"\\n\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Concatenate tout 'comment' text\n",
    "all_comments = ' '.join(df['comment_preprocessed'])\n",
    "\n",
    "# Word cloud pour l'ensemble\n",
    "wordcloud_all = WordCloud().generate(all_comments)\n",
    "\n",
    "# Subplots pour les notes\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Afficher word cloud pour toutes les observations\n",
    "axes[0].imshow(wordcloud_all, interpolation='bilinear')\n",
    "axes[0].set_title('Word Cloud for All Observations')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Pour chaque note\n",
    "for i, note in enumerate(range(5, 0, -1)):\n",
    "    comments = ' '.join(df[df['note'] == note]['comment_preprocessed'])\n",
    "    wordcloud_note = WordCloud().generate(comments)\n",
    "    axes[i+1].imshow(wordcloud_note, interpolation='bilinear')\n",
    "    axes[i+1].set_title('Note {}'.format(note))\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "# Pour cacher les extras word clouds, si besoin\n",
    "if len(axes) > 6:\n",
    "    for j in range(6, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idem pour full text \n",
    "\n",
    "all_comments = ' '.join(df['full_text'])\n",
    "\n",
    "wordcloud_all = WordCloud().generate(all_comments)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes[0].imshow(wordcloud_all, interpolation='bilinear')\n",
    "axes[0].set_title('Word Cloud for All Observations')\n",
    "axes[0].axis('off')\n",
    "\n",
    "for i, note in enumerate(range(5, 0, -1)):\n",
    "    comments = ' '.join(df[df['note'] == note]['full_text'])\n",
    "    wordcloud_note = WordCloud().generate(comments)\n",
    "    axes[i+1].imshow(wordcloud_note, interpolation='bilinear')\n",
    "    axes[i+1].set_title('Note {}'.format(note))\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "if len(axes) > 6:\n",
    "    for j in range(6, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "\n",
    "# Concatenate tous 'comment' text\n",
    "all_comments = ' '.join(df['comment_preprocessed'])\n",
    "\n",
    "# N-gram value\n",
    "n = 3  # 2-grams, tu mets 2, 3-grams tu mets 3, etc.\n",
    "\n",
    "# Word frequency all data\n",
    "all_comments_tokens = all_comments.split()\n",
    "ngrams_all = list(ngrams(all_comments_tokens, n))\n",
    "word_frequency_all = Counter(ngrams_all)\n",
    "\n",
    "# Sort N-grams\n",
    "sorted_word_frequency_all = sorted(word_frequency_all.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Freq pour chaque note (5 to 1)\n",
    "for note in range(5, 0, -1):\n",
    "    comments = ' '.join(df[df['note'] == note]['comment_preprocessed'])\n",
    "    comments_tokens = comments.split()\n",
    "    ngrams_note = list(ngrams(comments_tokens, n))\n",
    "    word_frequency_note = Counter(ngrams_note)\n",
    "\n",
    "    # Sort N-grams par freq pour chaque note\n",
    "    sorted_word_frequency_note = sorted(word_frequency_note.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Save format text file\n",
    "    with open(f\"word_frequency_note_{note}.txt\", \"w\", encoding='utf-8') as file:\n",
    "        for word, freq in sorted_word_frequency_note:\n",
    "            file.write(f\"{word}: {freq}\\n\")\n",
    "\n",
    "# Save format text file\n",
    "with open(\"word_frequency_all.txt\", \"w\", encoding='utf-8') as file:\n",
    "    for word, freq in sorted_word_frequency_all:\n",
    "        file.write(f\"{word}: {freq}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idem pour full text\n",
    "all_comments = ' '.join(df['full_text'])\n",
    "\n",
    "n = 2  \n",
    "\n",
    "all_comments_tokens = all_comments.split()\n",
    "ngrams_all = list(ngrams(all_comments_tokens, n))\n",
    "word_frequency_all = Counter(ngrams_all)\n",
    "\n",
    "sorted_word_frequency_all = sorted(word_frequency_all.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "for note in range(5, 0, -1):\n",
    "    comments = ' '.join(df[df['note'] == note]['full_text'])\n",
    "    comments_tokens = comments.split()\n",
    "    ngrams_note = list(ngrams(comments_tokens, n))\n",
    "    word_frequency_note = Counter(ngrams_note)\n",
    "\n",
    "    sorted_word_frequency_note = sorted(word_frequency_note.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    with open(f\"word_frequency_note_{note}.txt\", \"w\", encoding='utf-8') as file:\n",
    "        for word, freq in sorted_word_frequency_note:\n",
    "            file.write(f\"{word}: {freq}\\n\")\n",
    "\n",
    "\n",
    "with open(\"word_frequency_all.txt\", \"w\", encoding='utf-8') as file:\n",
    "    for word, freq in sorted_word_frequency_all:\n",
    "        file.write(f\"{word}: {freq}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec and toy deep learning model (LSTM-based recurrent neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['note', 'full_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['note', 'full_text']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train, validation & test sets\n",
    "df_text = df[['note', 'full_text']]\n",
    "train_df, temp_df = train_test_split(df_text, test_size=0.3, random_state=42) # reserve 30% for test+validation\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42) # split the 30% into 15% test and 15% validation\n",
    "\n",
    "# Preprocess (tokenization, padding, etc.)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['full_text'])\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['full_text'])\n",
    "val_sequences = tokenizer.texts_to_sequences(val_df['full_text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['full_text'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "max_len = 100  # Maximum sequence len\n",
    "train_data = pad_sequences(train_sequences, maxlen=max_len)\n",
    "val_data = pad_sequences(val_sequences, maxlen=max_len)\n",
    "test_data = pad_sequences(test_sequences, maxlen=max_len)\n",
    "\n",
    "num_classes = 5  \n",
    "\n",
    "train_labels = to_categorical(train_df['note'] - 1, num_classes=num_classes)  \n",
    "val_labels = to_categorical(val_df['note'] - 1, num_classes=num_classes)\n",
    "test_labels = to_categorical(test_df['note'] - 1, num_classes=num_classes)\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=train_sequences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train \n",
    "model.fit(train_data, train_labels, epochs=3, batch_size=32, validation_data=(val_data, val_labels))\n",
    "\n",
    "# Evaluate \n",
    "loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "model1.add(LSTM(64, return_sequences=True))\n",
    "model1.add(LSTM(64))\n",
    "model1.add(Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "model1.fit(train_data, train_labels, epochs=3, batch_size=32, validation_data=(val_data, val_labels))\n",
    "\n",
    "loss, accuracy = model1.evaluate(test_data, test_labels)\n",
    "print(f\"Model 1 - Test Loss: {loss:.4f}\")\n",
    "print(f\"Model 1 - Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "model2.add(GRU(128))\n",
    "model2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "model2.fit(train_data, train_labels, epochs=3, batch_size=32, validation_data=(val_data, val_labels))\n",
    "\n",
    "loss, accuracy = model2.evaluate(test_data, test_labels)\n",
    "print(f\"Model 2 - Test Loss: {loss:.4f}\")\n",
    "print(f\"Model 2 - Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "model3.add(Bidirectional(LSTM(64)))\n",
    "model3.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model3.summary()\n",
    "\n",
    "model3.fit(train_data, train_labels, epochs=3, batch_size=32, validation_data=(val_data, val_labels))\n",
    "\n",
    "loss, accuracy = model3.evaluate(test_data, test_labels)\n",
    "print(f\"Model 3 - Test Loss: {loss:.4f}\")\n",
    "print(f\"Model 3 - Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "model4.add(Conv1D(128, 5, activation='relu'))\n",
    "model4.add(MaxPooling1D(5))\n",
    "model4.add(LSTM(64))\n",
    "model4.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model4.summary()\n",
    "\n",
    "model4.fit(train_data, train_labels, epochs=3, batch_size=32, validation_data=(val_data, val_labels))\n",
    "\n",
    "loss, accuracy = model4.evaluate(test_data, test_labels)\n",
    "print(f\"Model 4 - Test Loss: {loss:.4f}\")\n",
    "print(f\"Model 4 - Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"reviews_takeaway.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Convert data à InputExample format\n",
    "class InputExample(object):\n",
    "    def __init__(self, guid, text_a, text_b, label):\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "def convert_data_to_examples(df, input_column, target_column):\n",
    "    examples = []\n",
    "    for i, row in df.iterrows():\n",
    "        guid = None\n",
    "        text_a = row[input_column]\n",
    "        text_b = None\n",
    "        label = row[target_column]\n",
    "        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    return examples\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, examples, tokenizer, max_length):\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.examples[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            example.text_a,\n",
    "            example.text_b,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids = torch.tensor(inputs[\"input_ids\"])\n",
    "        attention_mask = torch.tensor(inputs[\"attention_mask\"])\n",
    "        token_type_ids = torch.tensor(inputs[\"token_type_ids\"])\n",
    "        label = torch.tensor(example.label)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "# Split\n",
    "df['full_raw'] = 'TITRE: ' + df['title'] + ' |AND| ' + 'COMMENT: ' + df['comment']\n",
    "df_text = df[['note', 'full_raw']].copy()\n",
    "df_text['note'] = df_text['note'] - 1\n",
    "\n",
    "train_df, val_df = train_test_split(df_text, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert à InputExample format\n",
    "train_examples = convert_data_to_examples(train_df, 'full_raw', 'note')\n",
    "val_examples = convert_data_to_examples(val_df, 'full_raw', 'note')\n",
    "test_examples = convert_data_to_examples(test_df, 'full_raw', 'note')\n",
    "\n",
    "# Convert à PyTorch dataset format\n",
    "train_dataset = CustomDataset(train_examples, tokenizer, max_length=128)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(val_examples, tokenizer, max_length=128)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)\n",
    "\n",
    "# Freeze BERT layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Define optimizer, loss function, and metric\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, eps=1e-8)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Train\n",
    "num_epochs = 3 #à 10 epochs, ça overfit dès le 4e epoch\n",
    "#for epoch in range(num_epochs): #si sans tqdm\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    train_loss = total_loss / len(train_dataloader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_samples += labels.size(0)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_accuracy = val_correct / val_samples\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    print()\n",
    "    \n",
    "\n",
    "test_dataset = CustomDataset(test_examples, tokenizer, max_length=128)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "test_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_samples += labels.size(0)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_dataloader)\n",
    "test_accuracy = test_correct / test_samples\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "# Convert InputExample format\n",
    "def convert_data_to_examples(df, INPUT_COLUMN, TARGET_COLUMN):\n",
    "    InputExamples = df.apply(lambda x: InputExample(guid=None, \n",
    "                                                    text_a = x[INPUT_COLUMN], \n",
    "                                                    text_b = None,\n",
    "                                                    label = x[TARGET_COLUMN]), axis = 1)\n",
    "    return InputExamples\n",
    "\n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] \n",
    "    for e in examples:\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Split\n",
    "df['full_raw'] = 'TITRE: ' + df['title'] + ' |AND| ' + 'COMMENT: ' + df['comment']\n",
    "df_text = df[['note', 'full_raw']].copy()  # Using .copy() to avoid the SettingWithCopyWarning\n",
    "df_text['note'] = df_text['note'] - 1\n",
    "\n",
    "train_df, val_df = train_test_split(df_text, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert InputExample format\n",
    "train_InputExamples = convert_data_to_examples(train_df, 'full_raw', 'note')\n",
    "val_InputExamples = convert_data_to_examples(val_df, 'full_raw', 'note')\n",
    "test_InputExamples = convert_data_to_examples(test_df, 'full_raw', 'note')\n",
    "\n",
    "# Convert à tensorflow dataset format\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "val_data = convert_examples_to_tf_dataset(list(val_InputExamples), tokenizer)\n",
    "val_data = val_data.batch(32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5) \n",
    "\n",
    "# Freeze BERT layers\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "# Define optimizer, loss function and metric\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train\n",
    "history = model.fit(train_data, epochs=2, validation_data=val_data)\n",
    "\n",
    "\n",
    "test_data = convert_examples_to_tf_dataset(list(test_InputExamples), tokenizer)\n",
    "test_data = test_data.batch(32)\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(test_data)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    296/Unknown - 3120s 10s/step - loss: 1.0218 - accuracy: 0.5917"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)\n",
    "\n",
    "\n",
    "# Define optimizer, loss function and metric\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train\n",
    "history = model.fit(train_data, epochs=2, validation_data=val_data)\n",
    "\n",
    "\n",
    "test_data = convert_examples_to_tf_dataset(list(test_InputExamples), tokenizer)\n",
    "test_data = test_data.batch(32)\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(test_data)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CamemBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import CamembertTokenizer, TFCamembertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# CamemBERT tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Convert InputExample format\n",
    "def convert_data_to_examples(df, INPUT_COLUMN, TARGET_COLUMN):\n",
    "    InputExamples = df.apply(lambda x: InputExample(guid=None, \n",
    "                                                    text_a = x[INPUT_COLUMN], \n",
    "                                                    text_b = None,\n",
    "                                                    label = x[TARGET_COLUMN]), axis = 1)\n",
    "    return InputExamples\n",
    "\n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] \n",
    "    for e in examples:\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "# Split\n",
    "df['full_raw'] = 'TITRE: ' + df['title'] + ' |AND| ' + 'COMMENT: ' + df['comment']\n",
    "df_text = df[['note', 'full_raw']].copy()  # Using .copy() to avoid the SettingWithCopyWarning\n",
    "df_text['note'] = df_text['note'] - 1\n",
    "\n",
    "train_df, val_df = train_test_split(df_text, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert InputExample format\n",
    "train_InputExamples = convert_data_to_examples(train_df, 'full_raw', 'note')\n",
    "val_InputExamples = convert_data_to_examples(val_df, 'full_raw', 'note')\n",
    "test_InputExamples = convert_data_to_examples(test_df, 'full_raw', 'note')\n",
    "\n",
    "# Convert tensorflow dataset format\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "val_data = convert_examples_to_tf_dataset(list(val_InputExamples), tokenizer)\n",
    "val_data = val_data.batch(32)\n",
    "\n",
    "# Load pre-trained CamemBERT model\n",
    "model = TFCamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=5) \n",
    "\n",
    "# Freeze CamemBERT layers\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "# Define optimizer, loss function and metric\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train\n",
    "history = model.fit(train_data, epochs=2, validation_data=val_data)\n",
    "\n",
    "\n",
    "test_data = convert_examples_to_tf_dataset(list(test_InputExamples), tokenizer)\n",
    "test_data = test_data.batch(32)\n",
    "\n",
    "# Evaluate \n",
    "loss, accuracy = model.evaluate(test_data)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
